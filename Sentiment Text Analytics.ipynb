{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44900a56",
   "metadata": {},
   "source": [
    "# Text Analytics Pipeline — **Redacted, Annotated**\n",
    "This notebook merges multiple scripts into a single, **fully explained** workflow for product-review sentiment & tagging.\n",
    "All sensitive details are redacted and secrets must be provided via environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f24b08",
   "metadata": {},
   "source": [
    "## Pipeline Overview\n",
    "```\n",
    "[Ingest/Clean]\n",
    "     |\n",
    "     v\n",
    "[Sentiment Calculation (numeric)] -----> adds: sentiment_score/label\n",
    "     |\n",
    "     +--> [LLM Tagger (topics/aspects)] -> adds: tags/aspects, optional refined label\n",
    "             |\n",
    "             v\n",
    "     [Predicting Sentiment (ML model)] -> train/eval + batch inference\n",
    "             |\n",
    "             v\n",
    "     [Breakdown/Reporting] -> per-product/per-time summaries for dashboards\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc87a1",
   "metadata": {},
   "source": [
    "## sentiment_calculationNew.py — Purpose, I/O, and How-To\n",
    "\n",
    "**Purpose**: Compute **numeric sentiment scores** from text/ratings and map to labels.\n",
    "\n",
    "**Inputs (expected)**: `review_text` (and optionally `star_rating`).\n",
    "\n",
    "**Outputs**: Adds `sentiment_score` and `sentiment_label` columns.\n",
    "\n",
    "**When to run**: Run after cleaning to enrich dataset.\n",
    "\n",
    "**Key functions/classes (auto-extracted)**:\n",
    "- `def compute_sentiment_proportions(sentences_df)` — Aggregates sentiment counts per review from the sentences sheet.\n",
    "It extracts topics by building a mapping from the actual predicted sentiment column names.\n",
    "This approach handles extra spaces and any header discrepancies.\n",
    "\n",
    "Returns:\n",
    "  aggregated: dict keyed by review Key. Each value is a dict mapping topic to:\n",
    "              {\"POSITIVE\": count, \"NEGATIVE\": count, \"NEUTRAL\": count, \"count\": total, \"sentence\": example_sentence}\n",
    "  topics: sorted list of topics identified.\n",
    "- `def unpivot_reviews(reviews_df, aggregated, topics)` — Builds the unpivoted reviews dataset. Each review from the Reviews sheet is combined\n",
    "with the aggregated sentiment proportions (from the sentences sheet) on a per-topic basis.\n",
    "\n",
    "Meta-data from each review is extracted from the Reviews sheet, and dummy store columns\n",
    "are created. The sentiment proportions for each topic and a representative sentence are added.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_sentiment_proportions(sentences_df):\n",
    "    \"\"\"\n",
    "    Aggregates sentiment counts per review from the sentences sheet.\n",
    "    It extracts topics by building a mapping from the actual predicted sentiment column names.\n",
    "    This approach handles extra spaces and any header discrepancies.\n",
    "    \n",
    "    Returns:\n",
    "      aggregated: dict keyed by review Key. Each value is a dict mapping topic to:\n",
    "                  {\"POSITIVE\": count, \"NEGATIVE\": count, \"NEUTRAL\": count, \"count\": total, \"sentence\": example_sentence}\n",
    "      topics: sorted list of topics identified.\n",
    "    \"\"\"\n",
    "    # Define the known prefix and suffix in your sentiment column headers.\n",
    "    prefix = \"Predicted \"\n",
    "    suffix = \" Sentiment\"\n",
    "    \n",
    "    # Build a mapping from topic to its exact sentiment column header.\n",
    "    sentiment_cols = [\n",
    "        col for col in sentences_df.columns \n",
    "        if col.startswith(prefix) and col.endswith(suffix)\n",
    "    ]\n",
    "    topic_to_sentiment_col = {col[len(prefix):-len(suffix)]: col for col in sentiment_cols}\n",
    "    topics = sorted(topic_to_sentiment_col.keys())\n",
    "    \n",
    "    aggregated = {}\n",
    "    for _, row in tqdm(sentences_df.iterrows(), total=sentences_df.shape[0], desc=\"Aggregating sentences\"):\n",
    "        review_key = row[\"Key\"]\n",
    "        if review_key not in aggregated:\n",
    "            aggregated[review_key] = {}\n",
    "        \n",
    "        # Iterate over each topic derived from the mapping.\n",
    "        for topic in topics:\n",
    "            # Check if the sentence actually mentions the topic.\n",
    "            # Adjust this check if the data contains booleans or specific strings.\n",
    "            topic_val = str(row.get(topic, \"\")).strip().upper()\n",
    "            if topic_val != \"TRUE\":\n",
    "                continue\n",
    "\n",
    "            # Use the exact sentiment column name as determined in the mapping.\n",
    "            sentiment_col = topic_to_sentiment_col.get(topic)\n",
    "            if not sentiment_col:\n",
    "                continue\n",
    "\n",
    "            predicted = row.get(sentiment_col, \"\")\n",
    "            if pd.isnull(predicted):\n",
    "                continue\n",
    "\n",
    "            predicted = str(predicted).strip().upper()\n",
    "            if predicted not in [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]:\n",
    "                continue\n",
    "\n",
    "            # Initialize the topic dictionary for the review if necessary.\n",
    "            if topic not in aggregated[review_key]:\n",
    "                aggregated[review_key][topic] = {\n",
    "                    \"POSITIVE\": 0,\n",
    "                    \"NEGATIVE\": 0,\n",
    "                    \"NEUTRAL\": 0,\n",
    "                    \"count\": 0,\n",
    "                    \"sentence\": \"\"\n",
    "                }\n",
    "            aggregated[review_key][topic][predicted] += 1\n",
    "            aggregated[review_key][topic][\"count\"] += 1\n",
    "\n",
    "            # Store the first encountered sentence as a representative example.\n",
    "            if not aggregated[review_key][topic][\"sentence\"]:\n",
    "                aggregated[review_key][topic][\"sentence\"] = row.get(\"sentence\", \"\")\n",
    "                \n",
    "    return aggregated, topics\n",
    "\n",
    "def unpivot_reviews(reviews_df, aggregated, topics):\n",
    "    \"\"\"\n",
    "    Builds the unpivoted reviews dataset. Each review from the Reviews sheet is combined\n",
    "    with the aggregated sentiment proportions (from the sentences sheet) on a per-topic basis.\n",
    "    \n",
    "    Meta-data from each review is extracted from the Reviews sheet, and dummy store columns\n",
    "    are created. The sentiment proportions for each topic and a representative sentence are added.\n",
    "    \"\"\"\n",
    "    unpivoted_rows = []\n",
    "    \n",
    "    # Mapping for converting a single \"Store\" field into dummy columns.\n",
    "    store_mapping = {\n",
    "        \"Amazon\": \"Amazon.com\",\n",
    "        \"BestBuy\": \"BestBuy.com\",\n",
    "        \"Costco\": \"Costco.com\",\n",
    "        \"Homedepot\": \"HomeDepot.com\",\n",
    "        \"Lowes\": \"Lowes.com\",\n",
    "        \"Walmart\": \"Walmart.com\"\n",
    "    }\n",
    "    \n",
    "    for _, review in reviews_df.iterrows():\n",
    "        review_key = review[\"Key\"]\n",
    "        meta = {}\n",
    "        # Basic meta-data extracted from reviews.\n",
    "        meta[\"Key\"] = review.get(\"Key\", \"\")\n",
    "        meta[\"Promoted Review\"] = review.get(\"Promoted Review\", \"\")\n",
    "        meta[\"product_image\"] = review.get(\"image_list\", \"\")\n",
    "        # Format the review date to YYYY-MM-DD.\n",
    "        rev_date = review.get(\"review_date\", None)\n",
    "        if pd.notnull(rev_date):\n",
    "            try:\n",
    "                meta[\"review_date\"] = pd.to_datetime(rev_date, errors=\"coerce\").strftime('%Y-%m-%d')\n",
    "            except Exception:\n",
    "                meta[\"review_date\"] = \"\"\n",
    "        else:\n",
    "            meta[\"review_date\"] = \"\"\n",
    "        meta[\"star_rating\"] = review.get(\"star_rating\", \"\")\n",
    "        meta[\"review_title\"] = review.get(\"review_title\", \"\")\n",
    "        meta[\"review_text\"] = review.get(\"review_text\", \"\")\n",
    "        meta[\"Model_number\"] = review.get(\"Model_number\", \"\")\n",
    "        meta[\"Brand\"] = review.get(\"Brand\", \"\")\n",
    "        meta[\"Price\"] = review.get(\"Price\", \"\")\n",
    "        \n",
    "        # Create dummy store columns (set to 1 for matching store, 0 otherwise).\n",
    "        store_val = str(review.get(\"Store\", \"\")).strip()\n",
    "        dummy_stores = {out_key: 0 for out_key in store_mapping.values()}\n",
    "        for in_store, out_col in store_mapping.items():\n",
    "            if store_val.lower() == in_store.lower():\n",
    "                dummy_stores[out_col] = 1\n",
    "                break\n",
    "        meta.update(dummy_stores)\n",
    "        \n",
    "        # For each topic, add the sentiment proportions.\n",
    "        for topic in topics:\n",
    "            new_row = meta.copy()\n",
    "            new_row[\"Topic\"] = topic\n",
    "            if review_key in aggregated and topic in aggregated[review_key]:\n",
    "                data = aggregated[review_key][topic]\n",
    "                count = data.get(\"count\", 0)\n",
    "                if count > 0:\n",
    "                    new_row[\"Proportion Positive\"] = data.get(\"POSITIVE\", 0) / count\n",
    "                    new_row[\"Proportion Neutral\"] = data.get(\"NEUTRAL\", 0) / count\n",
    "                    new_row[\"Proportion Negative\"] = data.get(\"NEGATIVE\", 0) / count\n",
    "                else:\n",
    "                    new_row[\"Proportion Positive\"] = 0.0\n",
    "                    new_row[\"Proportion Neutral\"] = 0.0\n",
    "                    new_row[\"Proportion Negative\"] = 0.0\n",
    "                new_row[\"sentence\"] = data.get(\"sentence\", \"\")\n",
    "            else:\n",
    "                new_row[\"Proportion Positive\"] = 0.0\n",
    "                new_row[\"Proportion Neutral\"] = 0.0\n",
    "                new_row[\"Proportion Negative\"] = 0.0\n",
    "                new_row[\"sentence\"] = \"\"\n",
    "            \n",
    "            unpivoted_rows.append(new_row)\n",
    "    \n",
    "    unpivoted_df = pd.DataFrame(unpivoted_rows)\n",
    "    return unpivoted_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the Reviews and Sentences sheets from the Excel file.\n",
    "    file_path = \"/Users/eureka/Desktop/Predicting_Sentiment Combo.xlsx\"\n",
    "    reviews_df = pd.read_excel(file_path, sheet_name=\"Reviews\")\n",
    "    sentences_df = pd.read_excel(file_path, sheet_name=\"Sentences\")\n",
    "    \n",
    "    # Compute aggregated sentiment counts and sample sentence per review and topic.\n",
    "    aggregated, topics = compute_sentiment_proportions(sentences_df)\n",
    "    print(\"Identified topics:\", topics)\n",
    "    \n",
    "    # Build the unpivoted reviews dataset.\n",
    "    unpivoted_df = unpivot_reviews(reviews_df, aggregated, topics)\n",
    "    \n",
    "    # Export the unpivoted dataset to an Excel file.\n",
    "    output_excel = \"/Users/eureka/Desktop/Unpivoted Combo Review.xlsx\"\n",
    "    unpivoted_df.to_excel(output_excel, index=False)\n",
    "    print(\"Unpivoted reviews dataset saved to:\", output_excel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef463c",
   "metadata": {},
   "source": [
    "## GPT-TaggerNew.py — Purpose, I/O, and How-To\n",
    "\n",
    "**Purpose**: Use an **LLM API** to assign **topics/aspects** and optionally refine sentiment labels.\n",
    "\n",
    "**Inputs (expected)**: Cleaned `review_text` and metadata; requires API key via env.\n",
    "\n",
    "**Outputs**: Adds `tags`/`aspects` and possibly `sentiment_label_llm`.\n",
    "\n",
    "**When to run**: Run post-cleaning. Placeholders are redacted.\n",
    "\n",
    "**Key functions/classes (auto-extracted)**:\n",
    "- `def promotedTagger(data_frame)` — Special tag that detects if a review was part of a promotion.\n",
    "- `def predict_topics_with_gpt(topics, sentence, appliance_type)` — Uses OpenAI service to predict topics for the given sentence.\n",
    "- `def gpt_tagger_worker(n, q)` — No docstring\n",
    "- `def main(new_version_of_tag_file, reviews_df, sentences_df_original, output_file, appliance_type_original)` — No docstring\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Use your API key here – ensure it is valid!\n",
    "client = OpenAI(api_key=\"<REDACTED>\")\n",
    "\n",
    "def promotedTagger(data_frame):\n",
    "    \"\"\"Special tag that detects if a review was part of a promotion.\"\"\"\n",
    "    print(\"\\nTagging for the Promoted Review tag.\")\n",
    "    print(\"Searching for the following phrase:\")\n",
    "    print(\"'review was collected as part of a promotion'\")\n",
    "    promoted_keys = []\n",
    "    result = []\n",
    "    for index, row in tqdm(data_frame.iterrows(), total=data_frame.shape[0]):\n",
    "        row_tag = False\n",
    "        # Check if the review text contains the target phrase (case-insensitive)\n",
    "        if \"review was collected as part of a promotion\" in str(row['review_text']).lower():\n",
    "            row_tag = True\n",
    "            promoted_keys.append(row['Key'])\n",
    "        result.append(row_tag)\n",
    "    return {\"Promoted Review Bool List\": result, \"Promoted Keys\": promoted_keys}\n",
    "\n",
    "def predict_topics_with_gpt(topics, sentence, appliance_type):\n",
    "    \"\"\"\n",
    "    Uses OpenAI service to predict topics for the given sentence.\n",
    "    \"\"\"\n",
    "    model_name = \"gpt-4o\"\n",
    "    output = \"Not Available\"\n",
    "    \n",
    "    retries = 5\n",
    "    print(\"-\" * 80)\n",
    "    print(sentence)\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\",\n",
    "                     \"content\": f\"In the context of {appliance_type} online reviews, does the following sentence talk about any of the following topics {str(topics)}? Return only a list of the relevant topics from my list and nothing else. Sentence: {sentence}\"}\n",
    "                ],\n",
    "                max_tokens=50,\n",
    "                temperature=0,\n",
    "                timeout=20\n",
    "            )\n",
    "            output = response.choices[0].message.content\n",
    "            print(\"\\n\" + output + \"\\n\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            retries -= 1\n",
    "            if retries > 0:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"Wasn't able to detect the relevant topics. Assigning 'Not Available'\\n\")\n",
    "                print(e)\n",
    "    return output, model_name\n",
    "\n",
    "def gpt_tagger_worker(n, q):\n",
    "    global topics_only_list\n",
    "    global sentences_df\n",
    "    global appliance_type\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Try to get an item from the queue.\n",
    "            data = q.get(block=False, timeout=1)\n",
    "        except Exception as e:\n",
    "            # When queue is empty, the thread will exit.\n",
    "            print(\"Thread\", n, \"has joined\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            index = data[0]\n",
    "            row = data[1]\n",
    "            if len(str(row[\"sentence\"])) > 2:\n",
    "                if \"review was collected as part of a promotion\" in str(row[\"sentence\"]).lower():\n",
    "                    # If it is a promoted sentence, skip GPT tagging.\n",
    "                    gpts_response_for_topics = \"\"\n",
    "                else:\n",
    "                    gpts_response_for_topics, _ = predict_topics_with_gpt(topics_only_list, str(row[\"sentence\"]), appliance_type)\n",
    "            else:\n",
    "                print(\"-\" * 80)\n",
    "                print(str(row[\"sentence\"]), \"not long enough to predict topics\")\n",
    "                gpts_response_for_topics = \"\"\n",
    "            \n",
    "            # NEW FIX:\n",
    "            # If the GPT call failed (returned \"Not Available\"), mark this sentence as untagged and continue.\n",
    "            if gpts_response_for_topics == \"Not Available\":\n",
    "                sentences_df.at[index, \"Untagged\"] = True\n",
    "                q.task_done()\n",
    "                continue\n",
    "            \n",
    "            # Sort topics by length to avoid partial matching issues.\n",
    "            sorted_topics_only_list = sorted(topics_only_list, key=len, reverse=True)\n",
    "            topics_gpt_found = []\n",
    "            for topic in sorted_topics_only_list:\n",
    "                if topic in gpts_response_for_topics:\n",
    "                    sentences_df.at[index, topic] = True\n",
    "                    topics_gpt_found.append(topic)\n",
    "                    # Remove the matched topic from the response to avoid duplicate tagging.\n",
    "                    gpts_response_for_topics = gpts_response_for_topics.replace(topic, \"\")\n",
    "            \n",
    "            if topics_gpt_found == []:\n",
    "                sentences_df.at[index, \"Untagged\"] = True\n",
    "\n",
    "            print(\"Great Success\")\n",
    "            q.task_done()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Couldn't process data:\", data)\n",
    "            q.task_done()\n",
    "\n",
    "# Global variables used by threads\n",
    "things_to_tag_q = Queue(maxsize=0)\n",
    "topics_only_list = []\n",
    "sentences_df = pd.DataFrame()\n",
    "appliance_type = \"\"\n",
    "CONCURRENCY = 30\n",
    "\n",
    "def main(new_version_of_tag_file, reviews_df, sentences_df_original, output_file, appliance_type_original):\n",
    "    global things_to_tag_q, topics_only_list, sentences_df, appliance_type\n",
    "    things_to_tag_q = Queue(maxsize=0)\n",
    "    topics_only_list = []\n",
    "    sentences_df = sentences_df_original\n",
    "    appliance_type = appliance_type_original\n",
    "\n",
    "    try:\n",
    "        # Read the tag file – the Excel file headers correspond to the topics.\n",
    "        tag_topic_and_context_df = pd.read_excel(new_version_of_tag_file)\n",
    "        topics_only_list = tag_topic_and_context_df.columns.tolist()\n",
    "        \n",
    "        # Tag reviews that are part of a promotion.\n",
    "        promoted_tagger_dict = promotedTagger(reviews_df)\n",
    "        reviews_df[\"Promoted Review\"] = promoted_tagger_dict[\"Promoted Review Bool List\"]\n",
    "        sentences_df[\"Promoted Review\"] = False\n",
    "        \n",
    "        # Tag sentences from promoted reviews.\n",
    "        for index, row in sentences_df.iterrows():\n",
    "            if row[\"Key\"] in promoted_tagger_dict[\"Promoted Keys\"]:\n",
    "                sentences_df.at[index, \"Promoted Review\"] = True\n",
    "        \n",
    "        # Create new columns for each topic and for 'Untagged' in both dataframes.\n",
    "        for topic in topics_only_list + [\"Untagged\"]:\n",
    "            sentences_df[topic] = False\n",
    "            reviews_df[topic] = False\n",
    "        \n",
    "        # Enqueue each sentence row for GPT tagging.\n",
    "        for index, row in tqdm(sentences_df.iterrows(), total=sentences_df.shape[0]):\n",
    "            things_to_tag_q.put([index, row])\n",
    "            \n",
    "        threads = []\n",
    "        for i in range(CONCURRENCY):\n",
    "            thread = Thread(target=gpt_tagger_worker, args=(i, things_to_tag_q))\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "            \n",
    "        # Apply sentence-level topic tags to reviews by matching the Keys.\n",
    "        for index, row in reviews_df.iterrows():\n",
    "            matching_sentences = sentences_df[sentences_df[\"Key\"] == row[\"Key\"]]\n",
    "            for topic in topics_only_list + [\"Untagged\"]:\n",
    "                if True in matching_sentences[topic].tolist():\n",
    "                    reviews_df.at[index, topic] = True\n",
    "                    \n",
    "        # Export the updated data to an Excel file with two sheets.\n",
    "        output_file = output_file.split(\".xlsx\")[0] + \".xlsx\"\n",
    "        print(\"Exporting to:\", output_file)\n",
    "        writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "        writer.book.strings_to_urls = False\n",
    "        sentences_df.to_excel(writer, sheet_name='Sentences', index=False)\n",
    "        reviews_df.to_excel(writer, sheet_name='Reviews', index=False)\n",
    "        writer._save()\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        backup_file = output_file.split(\".xlsx\")[0] + \" Backup.xlsx\"\n",
    "        print(\"KeyboardInterrupt detected. Exporting to backup file:\", backup_file)\n",
    "        writer = pd.ExcelWriter(backup_file, engine='xlsxwriter')\n",
    "        writer.book.strings_to_urls = False\n",
    "        sentences_df.to_excel(writer, sheet_name='Sentences', index=False)\n",
    "        reviews_df.to_excel(writer, sheet_name='Reviews', index=False)\n",
    "        writer._save()\n",
    "    \n",
    "    return reviews_df, sentences_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tag_file = input(\"Enter name of tag file: \")\n",
    "    reviews_file_name = input(\"Enter name of reviews df file: \")\n",
    "    reviews_sheet_name = input(\"Enter name of reviews sheet: \")\n",
    "    sentences_file_name = input(\"Enter name of sentences df file: \")\n",
    "    sentences_sheet_name = input(\"Enter name of sentences sheet: \")\n",
    "    output_file = input(\"Enter name of output file: \")\n",
    "    appliance_type_original = input(\"Enter the appliance type: \")\n",
    "    \n",
    "    main(\n",
    "        tag_file,\n",
    "        pd.read_excel(reviews_file_name, sheet_name=reviews_sheet_name),\n",
    "        pd.read_excel(sentences_file_name, sheet_name=sentences_sheet_name),\n",
    "        output_file,\n",
    "        appliance_type_original\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636eb75",
   "metadata": {},
   "source": [
    "## predicting_sentimentNew.py — Purpose, I/O, and How-To\n",
    "\n",
    "**Purpose**: Train/evaluate a **ML classifier** for sentiment; supports batch inference.\n",
    "\n",
    "**Inputs (expected)**: Text + ground-truth label for training/validation.\n",
    "\n",
    "**Outputs**: Metrics (F1/precision/recall), confusion matrix, predictions, model artifact.\n",
    "\n",
    "**When to run**: Run for baseline/production classifier or alongside LLM.\n",
    "\n",
    "**Key functions/classes (auto-extracted)**:\n",
    "- **class PredictSentiment** — No docstring\n",
    "  - `def predict_with_gpt(self, sentence, topic)` — Uses OpenAI service to predict sentiment.\n",
    "- `def excelSheetToDf(fileName, sheetName)` — input: string fileName: name of excel file ending in .xlsx\n",
    "       string sheetName: name of the sheet you want to pull from fileName\n",
    "output: pandas dataframe df: uses the first row in the excel file as headers\n",
    "- `def predict_sentiment_worker(n, q)` — No docstring\n",
    "- `def main(reviews_df, sentences_df, output_file, topics_original)` — No docstring\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For every tagged sentence, this program makes a prediction whether\n",
    "it is positive, neutral or negative sentiment using only GPT.\n",
    "It then matches sentences to reviews with the same key (Unique Identifier)\n",
    "Josh Phillips, RAC co-op, Spring 2021 \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from openai import OpenAI\n",
    "\n",
    "# UPDATED: Directly create the OpenAI client using your API key.\n",
    "client = OpenAI(api_key=\"<REDACTED>\")\n",
    "\n",
    "class PredictSentiment:\n",
    "    def predict_with_gpt(self, sentence, topic):\n",
    "        \"\"\"\n",
    "        Uses OpenAI service to predict sentiment.\n",
    "        \"\"\"\n",
    "        retries = 5\n",
    "        output = \"NOT AVAILABLE\"\n",
    "        while retries > 0:\n",
    "            try:\n",
    "                if topic != \"Noise\":\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\",\n",
    "                             \"content\": f\"Analyze the following product review for topic: {topic} and determine if the sentiment regarding the topic is: POSITIVE, NEGATIVE or NEUTRAL. Return only a single word, either POSITIVE, NEGATIVE or NEUTRAL: {sentence}\"}\n",
    "                        ],\n",
    "                        max_tokens=10,\n",
    "                        temperature=0,\n",
    "                        timeout=10)\n",
    "                else:\n",
    "                    print(str(sentence))\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\",\n",
    "                             \"content\": f\"Analyze the following product review for topic: {topic} and determine if the sentiment regarding the topic is: POSITIVE, NEGATIVE or NEUTRAL. Quiet is POSITIVE and Loud or unusual noises is NEGATIVE. Return only a single word, either POSITIVE, NEGATIVE or NEUTRAL: {sentence}\"}\n",
    "                        ],\n",
    "                        max_tokens=10,\n",
    "                        temperature=0,\n",
    "                        timeout=10)\n",
    "                output = response.choices[0].message.content\n",
    "                print(\"\\n\" + output + \"\\n\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries -= 1\n",
    "                if retries > 0:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    print(\"Wasn't able to detect sentiment. Assigning Not Available\\n\")\n",
    "                    print(e)\n",
    "        return output.upper()\n",
    "\n",
    "# Global variables\n",
    "things_to_predict_q = Queue(maxsize=0)\n",
    "CONCURRENCY = 50\n",
    "topics = None\n",
    "sentiment_predictor = None\n",
    "data_frame = None\n",
    "\n",
    "def excelSheetToDf(fileName, sheetName):\n",
    "    \"\"\"\n",
    "    input: string fileName: name of excel file ending in .xlsx\n",
    "           string sheetName: name of the sheet you want to pull from fileName\n",
    "    output: pandas dataframe df: uses the first row in the excel file as headers\n",
    "    \"\"\"\n",
    "    excel_file = pd.ExcelFile(fileName)\n",
    "    df = excel_file.parse(sheetName)\n",
    "    return df\n",
    "\n",
    "def predict_sentiment_worker(n, q):\n",
    "    global topics, sentiment_predictor, data_frame\n",
    "    while True:\n",
    "        try:\n",
    "            data = q.get(block=False, timeout=1)\n",
    "            index = data[0]\n",
    "            row = data[1]\n",
    "\n",
    "            # Identify which topics are mentioned in the sentence (assumes a column per topic with a truthy value)\n",
    "            topics_mentioned_in_sentence = []\n",
    "            for topic in topics:\n",
    "                if row[topic]:\n",
    "                    topics_mentioned_in_sentence.append(topic)\n",
    "\n",
    "            # For each mentioned topic, predict sentiment using GPT.\n",
    "            for topic in topics_mentioned_in_sentence:\n",
    "                # Check for promotion-related text; if found, assign NEUTRAL\n",
    "                if \"this review was collected as part of a promotion\" in str(row['sentence']).lower():\n",
    "                    sentiment = \"NEUTRAL\"\n",
    "                else:\n",
    "                    sentiment = sentiment_predictor.predict_with_gpt(row['sentence'], topic)\n",
    "                data_frame.loc[index, 'Predicted ' + topic + ' Sentiment'] = sentiment\n",
    "\n",
    "            print(\"Great Success\")\n",
    "            q.task_done()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Thread\", n, \"has joined\")\n",
    "            break\n",
    "\n",
    "def main(reviews_df, sentences_df, output_file, topics_original):\n",
    "    global things_to_predict_q, topics, sentiment_predictor, data_frame\n",
    "\n",
    "    things_to_predict_q = Queue(maxsize=0)\n",
    "    topics = topics_original\n",
    "\n",
    "    # Create the predictor using GPT only.\n",
    "    sentiment_predictor = PredictSentiment()\n",
    "    data_frame = sentences_df\n",
    "\n",
    "    print(\"Loading the dataframe\")\n",
    "    # Create a new prediction column for each topic.\n",
    "    for topic in topics:\n",
    "        data_frame['Predicted ' + topic + ' Sentiment'] = 'Not Mentioned'\n",
    "\n",
    "    print(\"Loading up the queue of the sentences\")\n",
    "    for index, row in tqdm(data_frame.iterrows(), total=data_frame.shape[0]):\n",
    "        things_to_predict_q.put([index, row])\n",
    "\n",
    "    threads = []\n",
    "    for i in range(CONCURRENCY):\n",
    "        threads += [Thread(target=predict_sentiment_worker, args=(i, things_to_predict_q))]\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    # Matching the Sentences to the Reviews\n",
    "    print(\"Loading the entire reviews into a dataframe\")\n",
    "    review_data_frame = reviews_df\n",
    "\n",
    "    # Convert boolean columns in reviews to integers.\n",
    "    a_list_of_cols_to_be_int = []\n",
    "    for col in review_data_frame.columns:\n",
    "        if review_data_frame[col].dtype == bool:\n",
    "            a_list_of_cols_to_be_int.append(col)\n",
    "        print(a_list_of_cols_to_be_int)\n",
    "\n",
    "    for col in a_list_of_cols_to_be_int:\n",
    "        review_data_frame[col] = review_data_frame[col].astype(int)\n",
    "\n",
    "    print(\"Matching Sentences with Reviews\")\n",
    "    for index, topic in tqdm(enumerate(topics), total=len(topics)):\n",
    "        print(\"-\" * 100)\n",
    "        print(topic)\n",
    "        review_data_frame['Negatively Mentions ' + topic] = 0\n",
    "        review_data_frame['Neutrally Mentions ' + topic] = 0\n",
    "        review_data_frame['Positively Mentions ' + topic] = 0\n",
    "\n",
    "        print(data_frame[data_frame[topic] == 1])\n",
    "        for idx, row in data_frame[data_frame[topic] == 1].iterrows():\n",
    "            sentiment = row['Predicted ' + topic + ' Sentiment']\n",
    "            if sentiment == \"NEGATIVE\":\n",
    "                review_data_frame.loc[review_data_frame.Key == row['Key'], 'Negatively Mentions ' + topic] = 1\n",
    "            elif sentiment == \"NEUTRAL\":\n",
    "                review_data_frame.loc[review_data_frame.Key == row['Key'], 'Neutrally Mentions ' + topic] = 1\n",
    "            elif sentiment == \"POSITIVE\":\n",
    "                review_data_frame.loc[review_data_frame.Key == row['Key'], 'Positively Mentions ' + topic] = 1\n",
    "            else:\n",
    "                print(\"*\" * 100)\n",
    "                print(sentiment)\n",
    "\n",
    "    print(\"Exporting to:\", output_file)\n",
    "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "    writer.book.strings_to_urls = False\n",
    "\n",
    "    data_frame.to_excel(writer, sheet_name='Sentences', index=False)\n",
    "    review_data_frame.to_excel(writer, sheet_name='Reviews', index=False)\n",
    "    writer._save()\n",
    "\n",
    "    return review_data_frame, data_frame\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Prompt for the input files.\n",
    "    previous_file_name = input(\"Enter the name of the previous file (with Reviews and Sentences sheets): \")\n",
    "    output = input(\"Enter base name of output file (e.g., output.xlsx): \")\n",
    "    topics_file = input(\"Enter the name of the topics file (xlsx) with topics in the first row: \")\n",
    "\n",
    "    # Read topics from the first row of the provided Excel file.\n",
    "    topics_df = pd.read_excel(topics_file, header=None)\n",
    "    topics_original = topics_df.iloc[0].tolist()\n",
    "\n",
    "    reviews_df = pd.read_excel(previous_file_name, sheet_name=\"Reviews\")\n",
    "    sentences_df = pd.read_excel(previous_file_name, sheet_name=\"Sentences\")\n",
    "    main(reviews_df, sentences_df, output, topics_original)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4fb5e",
   "metadata": {},
   "source": [
    "## SentimentBreakdownNew.py — Purpose, I/O, and How-To\n",
    "\n",
    "**Purpose**: Aggregate labeled reviews to produce **sentiment/topic breakdowns** by product and time (counts, proportions, trends).\n",
    "\n",
    "**Inputs (expected)**: Reviews with: `review_id`, `product_id`, `created_at`, and `sentiment`/`topic` columns.\n",
    "\n",
    "**Outputs**: `sentiment_breakdown.csv` or DataFrame with grouped stats.\n",
    "\n",
    "**When to run**: Run after labeling steps to generate reports/dashboards.\n",
    "\n",
    "**Key functions/classes (auto-extracted)**:\n",
    "- `def read_input_file(file_name, sheet_name)` — Reads the input file. If the file is an Excel file (.xlsx), it uses the specified sheet name.\n",
    "If it's a CSV, it loads it directly.\n",
    "- `def filterDataFrame(df, column_name)` — Ensures the specified column is of string type and fills NaN values with empty strings.\n",
    "- `def lemmatise(sentence, stop_words)` — Tokenizes the sentence, removes punctuation and stop words, then lemmatizes the remaining words.\n",
    "- `def main(starting_df, output_file)` — No docstring\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fbbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Download required NLTK resources.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the sentiment analyzer.\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def read_input_file(file_name, sheet_name=None):\n",
    "    \"\"\"\n",
    "    Reads the input file. If the file is an Excel file (.xlsx), it uses the specified sheet name.\n",
    "    If it's a CSV, it loads it directly.\n",
    "    \"\"\"\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        excel_file = pd.ExcelFile(file_name)\n",
    "        df = excel_file.parse(sheet_name)\n",
    "    elif file_name.endswith('.csv'):\n",
    "        df = pd.read_csv(file_name)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use .xlsx or .csv.\")\n",
    "    return df\n",
    "\n",
    "def filterDataFrame(df, column_name):\n",
    "    \"\"\"\n",
    "    Ensures the specified column is of string type and fills NaN values with empty strings.\n",
    "    \"\"\"\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def lemmatise(sentence, stop_words):\n",
    "    \"\"\"\n",
    "    Tokenizes the sentence, removes punctuation and stop words, then lemmatizes the remaining words.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Remove apostrophes, convert to lowercase, and keep alphabetic words only.\n",
    "    words = [word.replace(\"'\", \"\") for word in words]\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    lem = WordNetLemmatizer()\n",
    "    words = [lem.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def main(starting_df, output_file):\n",
    "    # --- KEY COLUMN HANDLING (CHANGED) ---\n",
    "    # If a unique key column \"Key\" is not present, create one.\n",
    "    if 'Key' not in starting_df.columns:\n",
    "        if 'id_x' in starting_df.columns:\n",
    "            starting_df['Key'] = starting_df['id_x']\n",
    "        else:\n",
    "            starting_df['Key'] = starting_df.index\n",
    "\n",
    "    starting_df = filterDataFrame(starting_df, \"review_text\")\n",
    "    \n",
    "    # Compute overall sentiment for each review.\n",
    "    overall_sentiments = []\n",
    "    for index, review in tqdm(enumerate(starting_df['review_text']), total=starting_df.shape[0], desc=\"Calculating overall sentiment\"):\n",
    "        overall_sentiments.append(sid.polarity_scores(str(review))['compound'])\n",
    "    starting_df[\"review_sentiment_score\"] = overall_sentiments\n",
    "\n",
    "    # Prepare stop words.\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    # Remove common negations from stop words since they are important for sentiment.\n",
    "    negation = [\"mustn't\", \"aren't\", \"ain\", \"mightn\", \"needn\", \"wasn\", \"shan't\",\n",
    "                \"hadn\", \"mightn't\", \"isn't\", \"hasn\", \"shan\", \"hadn't\", \"shouldn\", \"needn't\",\n",
    "                \"doesn\", \"haven\", \"no\", \"wasn't\", \"mustn\", \"haven't\", \"didn\", \"weren't\",\n",
    "                \"wouldn't\", \"don't\", \"couldn't\", \"weren\", \"nor\", \"aren\", \"didn't\", \"wouldn\",\n",
    "                \"isn\", \"hasn't\", \"couldn\", \"don\", \"doesn't\", \"won't\", \"off\", \"on\", \"shouldn't\", \"not\",\n",
    "                \"does\", \"did\"]\n",
    "    for word in negation:\n",
    "        if word in stop_words:\n",
    "            stop_words.remove(word)\n",
    "    # Add additional words to ignore.\n",
    "    promoted = ['review', 'collected', 'part', 'promotion', 'rating', 'provided', 'verified', 'purchaser']\n",
    "    for word in promoted:\n",
    "        stop_words.add(word)\n",
    "        \n",
    "    # Prepare lists to store sentence-level data.\n",
    "    keys = []\n",
    "    sentences_only = []\n",
    "    lemmas = []\n",
    "    sentence_sentiments = []\n",
    "    row_list = []\n",
    "    \n",
    "    print(\"Performing sentiment analysis on sentences...\")\n",
    "    # Process each review in the dataset.\n",
    "    for index, row in tqdm(starting_df.iterrows(), total=starting_df.shape[0], desc=\"Breaking reviews into sentences\"):\n",
    "        review_text = str(row['review_text'])\n",
    "        # --- CHANGED CODE START ---\n",
    "        # Use regex to insert a space after punctuation only when not followed by another punctuation.\n",
    "        review_text = re.sub(r'([.!?])(?=[^\\s.!?])', r'\\1 ', review_text)\n",
    "        # --- CHANGED CODE END ---\n",
    "        \n",
    "        # Tokenize into sentences.\n",
    "        sentences = nltk.sent_tokenize(review_text)\n",
    "        for sentence in sentences:\n",
    "            # --- KEY COLUMN HANDLING (CHANGED) ---\n",
    "            # Instead of using row['id'], we use row['Key'] which we ensured exists.\n",
    "            keys.append(row['Key'])\n",
    "            sentences_only.append(sentence)\n",
    "            lemmas.append(lemmatise(sentence, stop_words))\n",
    "            sentence_sentiments.append(sid.polarity_scores(sentence)['compound'])\n",
    "            row_list.append(row.to_dict())\n",
    "    \n",
    "    # Create a DataFrame for the sentence-level breakdown.\n",
    "    sentences_df = pd.DataFrame(row_list)\n",
    "    # Insert new columns for sentence, lemma, and sentence sentiment score after the \"review_text\" column.\n",
    "    review_text_col_index = starting_df.columns.get_loc(\"review_text\")\n",
    "    sentences_df.insert(loc=review_text_col_index+1, column=\"sentence\", value=sentences_only)\n",
    "    sentences_df.insert(loc=review_text_col_index+2, column=\"lemma\", value=lemmas)\n",
    "    sentences_df.insert(loc=review_text_col_index+3, column=\"<REDACTED>\", value=sentence_sentiments)\n",
    "    \n",
    "    # Export the results to an Excel file with two sheets: one for Reviews and one for Sentences.\n",
    "    print(\"Exporting results to:\", output_file)\n",
    "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "    writer.book.strings_to_urls = False\n",
    "    starting_df.to_excel(writer, sheet_name='Reviews', index=False)\n",
    "    sentences_df.to_excel(writer, sheet_name='Sentences', index=False)\n",
    "    writer._save()\n",
    "    \n",
    "    return starting_df, sentences_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_name = input(\"Enter the name of the file containing the starting data frame (xlsx or csv): \")\n",
    "    output_file = input(\"Enter the name of the output Excel file to create (e.g., output.xlsx): \")\n",
    "    \n",
    "    # If an Excel file is provided, ask for the sheet name; otherwise, for CSV, skip it.\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        sheet_name = input(\"Enter the sheet name: \")\n",
    "        df = read_input_file(file_name, sheet_name)\n",
    "    elif file_name.endswith('.csv'):\n",
    "        df = read_input_file(file_name)\n",
    "    else:\n",
    "        print(\"Unsupported file type. Please provide a .csv or .xlsx file.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    main(df, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
